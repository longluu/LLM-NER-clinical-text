{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f1c2bf-26bd-4ef1-b70a-89c3096f121a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GatorTronS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdbc7d5-156e-4cb6-96c8-99ae607d9ee8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc20b64-f3b8-405a-a1ef-a1377db9f40a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/Tutorial-LLM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing the dataset ...\n",
      "/home/ec2-user/SageMaker/LLM-NER-clinical-text/data/public/MedMentions/preprocessed-data/\n",
      "The device to run the model: cuda\n",
      "Load the pretrained model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/Tutorial-LLM/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of MegatronBertForTokenClassification were not initialized from the model checkpoint at UFNLP/gatortrons and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 354.262059millions parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2,635\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,295\n",
      "  Number of trainable parameters = 354,262,059\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3295' max='3295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3295/3295 1:02:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.524100</td>\n",
       "      <td>0.416685</td>\n",
       "      <td>0.603839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.332700</td>\n",
       "      <td>0.401952</td>\n",
       "      <td>0.627280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.228800</td>\n",
       "      <td>0.429635</td>\n",
       "      <td>0.633531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.151200</td>\n",
       "      <td>0.470514</td>\n",
       "      <td>0.633126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.514751</td>\n",
       "      <td>0.633849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 878\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-659\n",
      "Configuration saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-659/config.json\n",
      "Model weights saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-659/model.safetensors\n",
      "tokenizer config file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-659/tokenizer_config.json\n",
      "Special tokens file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-659/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 878\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1318\n",
      "Configuration saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1318/config.json\n",
      "Model weights saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1318/model.safetensors\n",
      "tokenizer config file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1318/tokenizer_config.json\n",
      "Special tokens file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1318/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 878\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1977\n",
      "Configuration saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1977/config.json\n",
      "Model weights saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1977/model.safetensors\n",
      "tokenizer config file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1977/tokenizer_config.json\n",
      "Special tokens file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1977/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 878\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-2636\n",
      "Configuration saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-2636/config.json\n",
      "Model weights saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-2636/model.safetensors\n",
      "tokenizer config file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-2636/tokenizer_config.json\n",
      "Special tokens file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-2636/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 878\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-3295\n",
      "Configuration saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-3295/config.json\n",
      "Model weights saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-3295/model.safetensors\n",
      "tokenizer config file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-3295/tokenizer_config.json\n",
      "Special tokens file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-3295/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/checkpoint-1318 (score: 0.40195217728614807).\n",
      "Saving model checkpoint to /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/\n",
      "Configuration saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/config.json\n",
      "Model weights saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/model.safetensors\n",
      "tokenizer config file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tokenizer_config.json\n",
      "Special tokens file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.disable(logging.INFO) # disable INFO and DEBUG logging everywhere\n",
    "\n",
    "%run /home/ec2-user/SageMaker/LLM-NER-clinical-text/src/models/train_model.py \\\n",
    "--model_name 'UFNLP/gatortrons' \\\n",
    "--data_dir '/home/ec2-user/SageMaker/LLM-NER-clinical-text/data/public/MedMentions/preprocessed-data/' \\\n",
    "--batch_size 8 \\\n",
    "--num_train_epochs 5 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--weight_decay 0.01 \\\n",
    "--new_model_dir \"/home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/\" \\\n",
    "--path_umls_semtype '/home/ec2-user/SageMaker/LLM-NER-clinical-text/data/public/MedMentions/SemGroups_2018.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea059678-d54a-4ed2-82dc-e0323354d8fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate on test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe0794d8-5c85-4a96-87ab-2e19e1cf7b57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/Tutorial-LLM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing the dataset ...\n",
      "../data/public/MedMentions/preprocessed-data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.63101050849362,\n",
       " 'precision': 0.6714479559316552,\n",
       " 'recall': 0.6145681950682534,\n",
       " 'matthews_correlation': 0.7203455851532575}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.model import *\n",
    "\n",
    "# Load the model\n",
    "model_loader = ModelLoader('/home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/')\n",
    "\n",
    "# Evaluate the models on metrics\n",
    "scores = model_loader.evaluate_model(dataset_name='../data/public/MedMentions/preprocessed-data/',\\\n",
    "                                     path_umls_semtype='../data/public/MedMentions/SemGroups_2018.txt',\\\n",
    "                                     metric_names=['f1', 'precision', 'recall', 'matthews_correlation'])\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735a05b-0e15-4a21-be93-3761629c6753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a995cd00-d5fa-40bb-a211-cb56e6406e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "README.md: 100%|██████████| 21.0/21.0 [00:00<00:00, 3.65kB/s]\n",
      "model.safetensors: 100%|██████████| 1.42G/1.42G [00:34<00:00, 41.3MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/longluu/Clinical-NER-MedMentions-GatorTronS/commit/e6a951cf15bd341dacaf6c242100e784c7f0dee3', commit_message='--batch_size 4 --num_train_epochs 5 --learning_rate 5e-5 --weight_decay 0.01', commit_description='', oid='e6a951cf15bd341dacaf6c242100e784c7f0dee3', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the model to hub\n",
    "NER_model.push_to_hub(\"longluu/Clinical-NER-MedMentions-GatorTronS\", commit_message='--batch_size 4 --num_train_epochs 5 --learning_rate 5e-5 --weight_decay 0.01')\n",
    "tokenizer.push_to_hub(\"longluu/Clinical-NER-MedMentions-GatorTronS\", commit_message='--batch_size 4 --num_train_epochs 5 --learning_rate 5e-5 --weight_decay 0.01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b4a1d1-1aff-4960-adc1-020763075f74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# GatorTron-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b142ec-5299-4c7b-b542-fb11f221e020",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f9035-7155-4c45-917e-7eeac23f9a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.INFO) # disable INFO and DEBUG logging everywhere\n",
    "\n",
    "%run /home/ec2-user/SageMaker/LLM-NER-clinical-text/src/models/train_model.py \\\n",
    "--model_name 'UFNLP/gatortron-base' \\\n",
    "--data_dir '/home/ec2-user/SageMaker/LLM-NER-clinical-text/data/public/MedMentions/preprocessed-data/' \\\n",
    "--batch_size 8 \\\n",
    "--num_train_epochs 6 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--weight_decay 0.01 \\\n",
    "--new_model_dir \"/home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortron-base/\" \\\n",
    "--path_umls_semtype '/home/ec2-user/SageMaker/LLM-NER-clinical-text/data/public/MedMentions/SemGroups_2018.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a75064-71f1-4811-9ad8-0b8138d96013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tutorial-llm",
   "language": "python",
   "name": "conda_tutorial-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
