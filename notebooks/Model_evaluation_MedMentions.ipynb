{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f1c2bf-26bd-4ef1-b70a-89c3096f121a",
   "metadata": {},
   "source": [
    "# GatorTronS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdbc7d5-156e-4cb6-96c8-99ae607d9ee8",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc20b64-f3b8-405a-a1ef-a1377db9f40a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/Tutorial-LLM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing the dataset ...\n",
      "/home/ec2-user/SageMaker/LLM-NER-clinical-text/data/public/MedMentions/preprocessed-data/\n",
      "The device to run the model: cuda\n",
      "Load the pretrained model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/Tutorial-LLM/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of MegatronBertForTokenClassification were not initialized from the model checkpoint at UFNLP/gatortrons and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 354.262059millions parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2,635\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,295\n",
      "  Number of trainable parameters = 354,262,059\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3295' max='3295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3295/3295 1:02:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.524100</td>\n",
       "      <td>0.416685</td>\n",
       "      <td>0.603839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.332700</td>\n",
       "      <td>0.401952</td>\n",
       "      <td>0.627280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.228800</td>\n",
       "      <td>0.429635</td>\n",
       "      <td>0.633531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.151200</td>\n",
       "      <td>0.470514</td>\n",
       "      <td>0.633126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.514751</td>\n",
       "      <td>0.633849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 878\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-659\n",
      "Configuration saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-659/config.json\n",
      "Model weights saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-659/model.safetensors\n",
      "tokenizer config file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-659/tokenizer_config.json\n",
      "Special tokens file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-659/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 878\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1318\n",
      "Configuration saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1318/config.json\n",
      "Model weights saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1318/model.safetensors\n",
      "tokenizer config file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1318/tokenizer_config.json\n",
      "Special tokens file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1318/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 878\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1977\n",
      "Configuration saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1977/config.json\n",
      "Model weights saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1977/model.safetensors\n",
      "tokenizer config file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1977/tokenizer_config.json\n",
      "Special tokens file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-1977/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 878\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-2636\n",
      "Configuration saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-2636/config.json\n",
      "Model weights saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-2636/model.safetensors\n",
      "tokenizer config file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-2636/tokenizer_config.json\n",
      "Special tokens file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-2636/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 878\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-3295\n",
      "Configuration saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-3295/config.json\n",
      "Model weights saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-3295/model.safetensors\n",
      "tokenizer config file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-3295/tokenizer_config.json\n",
      "Special tokens file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tmp-checkpoint-3295/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/checkpoint-1318 (score: 0.40195217728614807).\n",
      "Saving model checkpoint to /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/\n",
      "Configuration saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/config.json\n",
      "Model weights saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/model.safetensors\n",
      "tokenizer config file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/tokenizer_config.json\n",
      "Special tokens file saved in /home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.disable(logging.INFO) # disable INFO and DEBUG logging everywhere\n",
    "\n",
    "%run /home/ec2-user/SageMaker/LLM-NER-clinical-text/src/models/train_model.py \\\n",
    "--model_name 'UFNLP/gatortrons' \\\n",
    "--data_dir '/home/ec2-user/SageMaker/LLM-NER-clinical-text/data/public/MedMentions/preprocessed-data/' \\\n",
    "--batch_size 4 \\\n",
    "--num_train_epochs 5 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--weight_decay 0.01 \\\n",
    "--new_model_dir \"/home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/\" \\\n",
    "--path_umls_semtype '/home/ec2-user/SageMaker/LLM-NER-clinical-text/data/public/MedMentions/SemGroups_2018.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea059678-d54a-4ed2-82dc-e0323354d8fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate on test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8314fec-802a-4949-8823-0db40aee4d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing the dataset ...\n",
      "../data/public/MedMentions/preprocessed-data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 878/878 [00:59<00:00, 14.75 examples/s]\n",
      "/home/ec2-user/SageMaker/custom-miniconda/miniconda/envs/Tutorial-LLM/lib/python3.9/site-packages/datasets/load.py:752: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.5524024274855981}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoConfig, AutoTokenizer, DataCollatorForTokenClassification\n",
    "from src.data.data_loader import *\n",
    "from datasets import load_metric\n",
    "from torch.nn.functional import cross_entropy\n",
    "import logging\n",
    "import torch\n",
    "logging.disable(logging.INFO) # disable INFO and DEBUG logging everywhere\n",
    "\n",
    "# Load the fine-tuned model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/')\n",
    "NER_model = AutoModelForTokenClassification.from_pretrained('/home/ec2-user/SageMaker/LLM-NER-clinical-text/models/medmentions/gatortrons/').to(device)\n",
    "\n",
    "# Load the data\n",
    "dataset_loader = DatasetLoader(dataset_name='../data/public/MedMentions/preprocessed-data/', path_umls_semtype='../data/public/MedMentions/SemGroups_2018.txt', model_name='UFNLP/gatortrons')\n",
    "data_medmentions, classmap, umls_label_code, tokenizer = dataset_loader.load_dataset()\n",
    "data_medmentions = data_medmentions.remove_columns(['Full Text', 'Entity Codes', 'tokens', 'ner_tags', 'token_labels'])\n",
    "\n",
    "# Create a collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "        \n",
    "# Make predictions on test data\n",
    "def forward_pass_with_label(batch):\n",
    "    # Convert dict of lists to list of dicts suitable for data collator\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    \n",
    "    # Pad inputs and labels and put all tensors on device\n",
    "    batch = data_collator(features)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        # Pass data through model  \n",
    "        output = NER_model(input_ids, attention_mask)\n",
    "        \n",
    "        # Logit.size: [batch_size, sequence_length, classes]\n",
    "        # Predict class with largest logit value on classes axis\n",
    "        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n",
    "\n",
    "    return {\"predicted_label\": predicted_label}\n",
    "\n",
    "prediction_results = data_medmentions['validation'].map(forward_pass_with_label, batched=True, batch_size=1)\n",
    "\n",
    "# Compute the metrics\n",
    "metric = load_metric('f1')\n",
    "for (prediction, label) in zip(prediction_results['predicted_label'], prediction_results['labels']):\n",
    "    metric.add_batch(predictions=prediction, references=label)\n",
    "    \n",
    "metric.compute(average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "093f99f9-39bd-4cda-aca5-184388bdc28b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'predicted_label'],\n",
       "    num_rows: 878\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735a05b-0e15-4a21-be93-3761629c6753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995cd00-d5fa-40bb-a211-cb56e6406e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "README.md: 100%|██████████| 21.0/21.0 [00:00<00:00, 3.65kB/s]\n",
      "model.safetensors:  38%|███▊      | 544M/1.42G [00:12<00:21, 40.1MB/s] "
     ]
    }
   ],
   "source": [
    "# Push the model to hub\n",
    "NER_model.push_to_hub(\"longluu/Clinical-NER-MedMentions-GatorTronS\", commit_message='--batch_size 4 --num_train_epochs 5 --learning_rate 5e-5 --weight_decay 0.01')\n",
    "tokenizer.push_to_hub(\"longluu/Clinical-NER-MedMentions-GatorTronS\", commit_message='--batch_size 4 --num_train_epochs 5 --learning_rate 5e-5 --weight_decay 0.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f9035-7155-4c45-917e-7eeac23f9a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom (Tutorial-LLM)",
   "language": "python",
   "name": "tutorial-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
